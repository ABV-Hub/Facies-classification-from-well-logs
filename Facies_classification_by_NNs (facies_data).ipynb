{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense,Dropout,Activation,Conv1D,MaxPooling1D,Flatten,MaxPool1D,BatchNormalization,LeakyReLU\n",
    "from keras.callbacks import ReduceLROnPlateau,TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adamax,Adagrad\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors =  ['#F56F45', '#FA7E7E','#8C8456','#8F6A9E',\n",
    "       '#226854','#9EDAAB', '#7C97FC', '#25C4EF', '#A7D909']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('facies_data.csv')\n",
    "def one_hot_enc(y):\n",
    "    classes = sorted(list(set(y)))\n",
    "    y_enc = np.zeros((y.shape[0],len(classes)),dtype=\"float32\")\n",
    "    for index,label in enumerate(y):\n",
    "        y_enc[index,classes.index(label)] = 1.\n",
    "    return y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[feature_names].values  # features\n",
    "print(X.shape)\n",
    "y = data['Facies'].values  # labels\n",
    "well = data['Well Name'].values\n",
    "\n",
    "depth = data['Depth'].values\n",
    "#nan_idx = np.any(np.isnan(X), axis=1)\n",
    "#X = X[np.logical_not(nan_idx), :]\n",
    "\n",
    "\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "\n",
    "max_values_x = np.amax(X,axis=0)#normalization\n",
    "X/=np.amax(X,axis=0)#normalization\n",
    "\n",
    "print (X.shape)\n",
    "print (y[50])\n",
    "#y = y[np.logical_not(nan_idx)]\n",
    "classes = set(y)\n",
    "print (classes)\n",
    "y = one_hot_enc(y)\n",
    "print (y[50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN\n",
    "units = 256\n",
    "model = Sequential([\n",
    "    \n",
    "    Dense(units,input_shape=(7,)),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(units),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(classes)),\n",
    "    Activation(\"softmax\"),\n",
    "    \n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',optimizer=\"adagrad\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y,validation_split=0.2,batch_size=10,epochs=400,callbacks=[checkpoint,tbCallBack],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 256\n",
    "model = Sequential([\n",
    "    \n",
    "    Dense(units,input_shape=(7,)),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(units),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(classes)),\n",
    "    Activation(\"softmax\"),\n",
    "    \n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',optimizer=\"adagrad\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y,validation_split=0.2,batch_size=10,epochs=400,callbacks=[checkpoint,tbCallBack],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "history = cnn_model.fit(X,y,validation_split=0.2,batch_size = 10,epochs=500,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'],color='magenta')\n",
    "plt.plot(history.history['val_loss'],color='darkcyan')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc='best')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['acc'],color='turquoise')\n",
    "plt.plot(history.history['val_acc'],color='mediumslateblue')\n",
    "plt.title('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train','test'],loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "model = Sequential([\n",
    "    \n",
    "    Conv1D(64, 2, strides=1,input_shape =(7,1)), \n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    #MaxPool1D(2,1),\n",
    "   \n",
    "\n",
    "    Conv1D(64, 2, strides=1,padding=\"same\"), \n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    #MaxPool1D(2,1),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(64,2,strides=1,padding=\"same\"),\n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(3,1),\n",
    "   \n",
    "   \n",
    "    Conv1D(128,2,strides=1,padding=\"same\"),\n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    #MaxPooling1D(2,1),\n",
    "    \n",
    "    \n",
    "    Conv1D(128,2,strides=1,padding=\"same\"),\n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(128,1,strides=1,padding=\"same\"),\n",
    "    Activation(\"relu\"), \n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(1,1),\n",
    "    \n",
    "    \n",
    "    Flatten(), \n",
    "    Dense(2500),\n",
    "    Activation(\"relu\"),\n",
    "    BatchNormalization(),\n",
    "   \n",
    "    Dropout(0.3),\n",
    "    Dense(1500),\n",
    "    Activation(\"relu\"),\n",
    "    \n",
    "    Dense(len(classes)),\n",
    "    Activation(\"softmax\"),\n",
    "    \n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=\"adagrad\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"weights/weights-488-0.74.h5\")\n",
    "filepath = \"weights/weights_NOT_ok_-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "tbCallBack = TensorBoard(log_dir = 'Graph/CNN13_graph_not_ok', histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                     save_weights_only=False)\n",
    "model.fit(X_train,y_train,shuffle=True,validation_data = (X_test, y_test),batch_size=64,epochs=2800,callbacks=[checkpoint,tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE HERE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_not_ok.pickle', 'rb') as f:\n",
    "    X_train,y_train, X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('C:/w_oknotok/weights_NOT_ok_-1403-0.75.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = X_test.reshape((830, 7,1))\n",
    "pred = model2.predict_classes(X_test)\n",
    "y_test_dec = np.argmax(y_test,axis=1)\n",
    "accurc = accuracy_score(y_test_dec,pred)\n",
    "f1_ = f1_score(y_test_dec,pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481927710843373"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurc\n",
    "# 0.7481927710843373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7520486563765958"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_\n",
    "#0.7520486563765958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_facies = np.array([[1,2], [1,2,3], [2,3], [4,5], [4,5,6], [5,6,7], [6,7,8], [6,7,8,9], [7,8,9]])\n",
    "p = 0\n",
    "\n",
    "for i in range(1, len(pred)):\n",
    "    if (pred[i] == adjacent_facies[pred[i]-1]).any():\n",
    "        p = p + 1\n",
    "accuracy_adj = p/float(len(pred))\n",
    "print('Adjacent facies classification accuracy = %f' % accuracy_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = {}\n",
    "Recall = {}\n",
    "y_test1_ = list(y_test)\n",
    "for j in range(1, 10):\n",
    "    TP,FP,FN,TN = 0,0,0,0\n",
    "    for i in range(1, len(pred)):\n",
    "        if ((y_test_dec[i] == j) and (pred[i] == j)):\n",
    "            TP += 1\n",
    "        if ((y_test_dec[i] != j) and (pred[i] == j)):\n",
    "            FP += 1\n",
    "        if ((y_test_dec[i] == j) and (pred[i] != j)):\n",
    "            FN += 1\n",
    "        if ((y_test_dec[i] != j) and (pred[i] != j)):\n",
    "            TN += 1   \n",
    "    if (TP+FP == 0):\n",
    "        Precision[j] = 0\n",
    "    else:\n",
    "        Precision[j] = float(TP)/(TP+FP)\n",
    "    if (TP+FN == 0):\n",
    "        Recall[j] = 0\n",
    "    else:\n",
    "        Recall[j] = float(TP)/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_dec,pred\n",
    "# Считаем по классам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\eto_CNN\\weights-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "tbCallBack = TensorBoard(log_dir = 'C:\\eto_CNN_graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                     save_weights_only=False)\n",
    "\n",
    "model.fit(X,y,validation_split=0.2,batch_size=10,epochs=500,callbacks=[checkpoint,tbCallBack],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\eto_CNN2\\weights-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "tbCallBack = TensorBoard(log_dir = 'C:\\eto_CNN_graph2', histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                     save_weights_only=False)\n",
    "\n",
    "model.fit(X,y,validation_split=0.2,batch_size=10,epochs=500,callbacks=[checkpoint,tbCallBack],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=2)\n",
    "model = Sequential([\n",
    "    \n",
    "    Conv1D(32, 4, strides=1,input_shape =(7,1),padding=\"same\"), \n",
    "    Activation(\"relu\"), \n",
    "   \n",
    "    Dropout(0.3),\n",
    "   \n",
    "    Conv1D(64, 4, strides=1,padding=\"same\"),\n",
    "    Activation(\"relu\"),\n",
    "    \n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(128, 4, strides=1,padding=\"same\"), \n",
    "    Activation(\"relu\"), \n",
    "    \n",
    "   \n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(), \n",
    "    Dense(256),\n",
    "    Activation(\"relu\"),\n",
    "   \n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(len(classes)),\n",
    "    Activation(\"softmax\"),\n",
    "    \n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=\"rmsprop\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\cnn_1d\\weights-{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "tbCallBack = TensorBoard(log_dir = 'C:\\cnn1d_graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                     save_weights_only=False)\n",
    "\n",
    "model.fit(X,y,validation_split=0.2,batch_size=10,epochs=4000,callbacks=[checkpoint,tbCallBack],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.callbacks.History"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
